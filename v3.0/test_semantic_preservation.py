"""
Test CrÃ­tico: Â¿Aurora preserva relaciones semÃ¡nticas reales?

Objetivo:
    Probar si la codificaciÃ³n FFE mantiene la estructura semÃ¡ntica
    de embeddings producidos por modelos de lenguaje reales.
    
Test de Ã‰xito:
    - Frases similares â†’ tensores similares
    - Frases diferentes â†’ tensores diferentes
    - Distancia semÃ¡ntica original â‰ˆ distancia tensorial Aurora
"""

import numpy as np
from typing import List, Tuple
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENCODER FFE (Python)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FFEEncoder:
    """Codifica embeddings continuos â†’ tensores FFE discretos (1/0/-1)"""
    
    def __init__(self, embedding_dim: int = 384, n_dims_pca: int = 81):
        """
        Args:
            embedding_dim: DimensiÃ³n del embedding de entrada
            n_dims_pca: DimensiÃ³n tras PCA (debe ser mÃºltiplo de 3)
        """
        self.embedding_dim = embedding_dim
        self.n_dims_pca = n_dims_pca
        
        # Verificar que n_dims_pca es mÃºltiplo de 3
        assert n_dims_pca % 3 == 0, "n_dims_pca debe ser mÃºltiplo de 3"
        
        # Componentes de transformaciÃ³n
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=n_dims_pca)
        self.fitted = False
        
        # Umbrales de cuantizaciÃ³n (aprenden durante fit)
        self.upper_threshold = None  # valores > umbral_alto â†’ 1
        self.lower_threshold = None  # valores < umbral_bajo â†’ 0
    
    def fit(self, embeddings: np.ndarray):
        """Aprende la transformaciÃ³n desde embeddings continuos"""
        # Normalizar
        scaled = self.scaler.fit_transform(embeddings)
        
        # Reducir dimensionalidad preservando varianza
        reduced = self.pca.fit_transform(scaled)
        
        # Aprender umbrales de cuantizaciÃ³n ADAPTATIVOS
        # Usar desviaciÃ³n estÃ¡ndar en vez de percentiles fijos
        std_dev = np.std(reduced)
        mean_val = np.mean(reduced)
        
        # Umbrales mÃ¡s conservadores (Â±0.5 std)
        self.lower_threshold = mean_val - 0.5 * std_dev
        self.upper_threshold = mean_val + 0.5 * std_dev
        
        self.fitted = True
        
        print(f"[FFEEncoder] Entrenado:")
        print(f"  Varianza preservada: {self.pca.explained_variance_ratio_.sum():.3f}")
        print(f"  Mean: {mean_val:.3f} | Std: {std_dev:.3f}")
        print(f"  Umbrales: {self.lower_threshold:.3f} / {self.upper_threshold:.3f}")
    
    def encode(self, embedding: np.ndarray) -> np.ndarray:
        """Codifica un embedding â†’ tensor FFE de trits"""
        assert self.fitted, "Debe llamar a fit() primero"
        
        # Transformar
        scaled = self.scaler.transform(embedding.reshape(1, -1))
        reduced = self.pca.transform(scaled).flatten()
        
        # Cuantizar a trits {1, 0, -1}
        tensor = np.zeros_like(reduced, dtype=np.int8)
        tensor[reduced > self.upper_threshold] = 1
        tensor[reduced < self.lower_threshold] = 0
        tensor[(reduced >= self.lower_threshold) & (reduced <= self.upper_threshold)] = -1
        
        return tensor
    
    def decode(self, tensor: np.ndarray) -> np.ndarray:
        """Reconstruye embedding aproximado desde tensor FFE"""
        assert self.fitted, "Debe llamar a fit() primero"
        
        # Dequantizar (trits â†’ valores continuos aproximados)
        continuous = np.zeros_like(tensor, dtype=np.float32)
        continuous[tensor == 1] = self.upper_threshold + 0.5
        continuous[tensor == 0] = self.lower_threshold - 0.5
        continuous[tensor == -1] = (self.upper_threshold + self.lower_threshold) / 2
        
        # Invertir PCA y normalizaciÃ³n
        restored_pca = self.pca.inverse_transform(continuous.reshape(1, -1))
        restored = self.scaler.inverse_transform(restored_pca).flatten()
        
        return restored


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MÃ‰TRICAS DE SIMILITUD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Similitud coseno entre dos vectores"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)

def hamming_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Similitud Hamming entre dos tensores FFE (trits)"""
    matches = np.sum(a == b)
    return matches / len(a)

def triadic_distance(a: np.ndarray, b: np.ndarray) -> float:
    """Distancia triÃ¡dica (cuenta solo diferencias en valores no-null)"""
    # Solo comparar donde ambos NO son null
    valid_mask = (a != -1) & (b != -1)
    if np.sum(valid_mask) == 0:
        return 1.0  # MÃ¡xima distancia si todo es null
    
    diff = np.sum(a[valid_mask] != b[valid_mask])
    return diff / np.sum(valid_mask)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST CRÃTICO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_semantic_preservation():
    """
    TEST CRÃTICO: Â¿Los tensores FFE preservan relaciones semÃ¡nticas?
    
    HipÃ³tesis:
        Si dos frases son semÃ¡nticamente similares en el espacio continuo,
        sus tensores FFE tambiÃ©n deben ser similares en el espacio discreto.
    """
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  TEST CRÃTICO: PreservaciÃ³n de Relaciones SemÃ¡nticas            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # â”â”â” PASO 1: Generar embeddings simulados (MiniLM-L6-v2 â†’ 384 dims) â”â”â”
    print("â”â”â” PASO 1: Generando embeddings simulados â”â”â”")
    print("(Simulando sentence-transformers/all-MiniLM-L6-v2)\n")
    
    # Frases de prueba (AMPLIADAS: necesitamos >= 81 muestras para PCA a 81 dims)
    frases_base = [
        "El gato duerme en el sofÃ¡",           # 0: tema gatos-dormir
        "Un felino descansa sobre el sillÃ³n",  # 1: tema gatos-dormir (similar a 0)
        "El perro corre en el parque",         # 2: tema perros-correr
        "Un can trota por el jardÃ­n",          # 3: tema perros-correr (similar a 2)
    ]
    
    # Simular embeddings (en producciÃ³n usar SentenceTransformer real)
    np.random.seed(42)
    
    # Crear embeddings base con estructura semÃ¡ntica implÃ­cita
    base_cat_sleep = np.random.randn(384) * 0.5
    base_dog_run = np.random.randn(384) * 0.5
    
    # Generar 100 variaciones para tener suficientes datos para PCA
    embeddings_list = []
    frases = []
    
    for i in range(25):  # 25 variaciones de cada tipo â†’ 100 total
        # Variaciones de gato-dormir
        embeddings_list.append(base_cat_sleep + np.random.randn(384) * 0.15)
        frases.append(f"{frases_base[0]} (var {i})")
        
        embeddings_list.append(base_cat_sleep + np.random.randn(384) * 0.15)
        frases.append(f"{frases_base[1]} (var {i})")
        
        # Variaciones de perro-correr
        embeddings_list.append(base_dog_run + np.random.randn(384) * 0.15)
        frases.append(f"{frases_base[2]} (var {i})")
        
        embeddings_list.append(base_dog_run + np.random.randn(384) * 0.15)
        frases.append(f"{frases_base[3]} (var {i})")
    
    embeddings = np.array(embeddings_list)
    
    print(f"Embeddings generados: {embeddings.shape}")
    print(f"DimensiÃ³n: {embeddings.shape[1]}")
    print(f"Total muestras: {len(frases)} (necesarias >= 81 para PCA)\n")
    
    # â”â”â” PASO 2: Calcular similitudes en espacio continuo â”â”â”
    print("â”â”â” PASO 2: Similitudes en espacio continuo (baseline) â”â”â”\n")
    
    # Usar las primeras 4 para comparaciÃ³n (representativas de cada grupo)
    test_indices = [0, 1, 50, 51]  # 0,1=gato-dormir | 50,51=perro-correr
    test_frases = [frases[i] for i in test_indices]
    test_embeddings = embeddings[test_indices]
    
    pairs = [
        (0, 1, "Similar (gato-dormir vs gato-dormir)"),
        (2, 3, "Similar (perro-correr vs perro-correr)"),
        (0, 2, "Diferente (gato-dormir vs perro-correr)"),
        (1, 3, "Diferente (gato-dormir vs perro-correr)"),
    ]
    
    print("Similitud Coseno (espacio continuo):")
    cosine_scores = {}
    for i, j, desc in pairs:
        sim = cosine_similarity(test_embeddings[i], test_embeddings[j])
        cosine_scores[(i, j)] = sim
        print(f"  [{i}â†”{j}] {desc:45s} â†’ {sim:.4f}")
    
    print()
    
    # â”â”â” PASO 3: Entrenar encoder FFE â”â”â”
    print("â”â”â” PASO 3: Entrenando FFE Encoder â”â”â”\n")
    
    encoder = FFEEncoder(embedding_dim=384, n_dims_pca=81)
    encoder.fit(embeddings)
    
    print()
    
    # â”â”â” PASO 4: Codificar a tensores FFE â”â”â”
    print("â”â”â” PASO 4: Codificando a tensores FFE â”â”â”\n")
    
    tensores = [encoder.encode(emb) for emb in test_embeddings]
    
    print("Tensores FFE generados:")
    for i, tensor in enumerate(tensores):
        nulls = np.sum(tensor == -1)
        ones = np.sum(tensor == 1)
        zeros = np.sum(tensor == 0)
        print(f"  [{i}] \"{test_frases[i][:45]}...\"")
        print(f"      DistribuciÃ³n: 1={ones} | 0={zeros} | N={nulls}")
    
    print()
    
    # â”â”â” PASO 5: Calcular similitudes en espacio FFE â”â”â”
    print("â”â”â” PASO 5: Similitudes en espacio FFE (discreto) â”â”â”\n")
    
    print("Similitud Hamming (espacio FFE):")
    hamming_scores = {}
    for i, j, desc in pairs:
        sim = hamming_similarity(tensores[i], tensores[j])
        hamming_scores[(i, j)] = sim
        print(f"  [{i}â†”{j}] {desc:45s} â†’ {sim:.4f}")
    
    print()
    
    print("Distancia TriÃ¡dica (ignora nulls):")
    triadic_scores = {}
    for i, j, desc in pairs:
        dist = triadic_distance(tensores[i], tensores[j])
        sim = 1.0 - dist  # Convertir a similitud
        triadic_scores[(i, j)] = sim
        print(f"  [{i}â†”{j}] {desc:45s} â†’ {sim:.4f}")
    
    print()
    
    # â”â”â” PASO 6: VERIFICACIÃ“N CRÃTICA â”â”â”
    print("â”â”â” PASO 6: VERIFICACIÃ“N CRÃTICA â”â”â”\n")
    
    # Criterio de Ã©xito:
    # - Pares similares deben tener similitud > pares diferentes
    # - Tanto en espacio continuo como en espacio FFE
    
    similar_pairs = [(0, 1), (2, 3)]
    different_pairs = [(0, 2), (1, 3)]
    
    print("ComparaciÃ³n de similitudes:\n")
    
    # Promedios
    avg_cosine_similar = np.mean([cosine_scores[p] for p in similar_pairs])
    avg_cosine_different = np.mean([cosine_scores[p] for p in different_pairs])
    
    avg_hamming_similar = np.mean([hamming_scores[p] for p in similar_pairs])
    avg_hamming_different = np.mean([hamming_scores[p] for p in different_pairs])
    
    avg_triadic_similar = np.mean([triadic_scores[p] for p in similar_pairs])
    avg_triadic_different = np.mean([triadic_scores[p] for p in different_pairs])
    
    print(f"Espacio Continuo (Coseno):")
    print(f"  Pares similares:    {avg_cosine_similar:.4f}")
    print(f"  Pares diferentes:   {avg_cosine_different:.4f}")
    print(f"  SeparaciÃ³n:         {avg_cosine_similar - avg_cosine_different:.4f}")
    
    print(f"\nEspacio FFE (Hamming):")
    print(f"  Pares similares:    {avg_hamming_similar:.4f}")
    print(f"  Pares diferentes:   {avg_hamming_different:.4f}")
    print(f"  SeparaciÃ³n:         {avg_hamming_similar - avg_hamming_different:.4f}")
    
    print(f"\nEspacio FFE (TriÃ¡dico):")
    print(f"  Pares similares:    {avg_triadic_similar:.4f}")
    print(f"  Pares diferentes:   {avg_triadic_different:.4f}")
    print(f"  SeparaciÃ³n:         {avg_triadic_similar - avg_triadic_different:.4f}")
    
    print()
    
    # â”â”â” VEREDICTO â”â”â”
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  VEREDICTO                                                        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # Test 1: Â¿Se preserva el orden? (USAR TRIÃDICA, no Hamming)
    test1_pass = (avg_triadic_similar > avg_triadic_different)
    
    # Test 2: Â¿La separaciÃ³n es significativa?
    test2_pass = (avg_triadic_similar - avg_triadic_different) > 0.05
    
    # Test 3: Â¿Los nulls estÃ¡n balanceados? (no todos null, no cero nulls)
    avg_nulls = np.mean([np.sum(t == -1) for t in tensores])
    test3_pass = (10 < avg_nulls < 70)  # Entre 12% y 86% nulls
    
    print(f"âœ“ Test 1 - Orden preservado (TriÃ¡dico):       {'âœ… PASS' if test1_pass else 'âŒ FAIL'}")
    print(f"âœ“ Test 2 - SeparaciÃ³n significativa (>0.05):  {'âœ… PASS' if test2_pass else 'âŒ FAIL'}")
    print(f"âœ“ Test 3 - Nulls balanceados (10-70):         {'âœ… PASS' if test3_pass else 'âŒ FAIL'} (avg={avg_nulls:.1f})")
    
    print()
    
    if test1_pass and test2_pass and test3_pass:
        print("ğŸŒŸ RESULTADO: PRESERVACIÃ“N SEMÃNTICA EXITOSA")
        print("\nLos tensores FFE mantienen la estructura semÃ¡ntica original.")
        print("Aurora puede operar sobre significado real, no solo nÃºmeros.")
        print("\nPrÃ³ximo paso: Integrar con LLM real (sentence-transformers)")
    else:
        print("âš ï¸ RESULTADO: PRESERVACIÃ“N PARCIAL O FALLIDA")
        print("\nAjustar hiperparÃ¡metros:")
        print("  - Aumentar n_dims_pca")
        print("  - Cambiar umbrales de cuantizaciÃ³n")
        print("  - Probar con embeddings reales (no simulados)")
    
    print()
    
    # â”â”â” INFORMACIÃ“N ADICIONAL â”â”â”
    print("â”â”â” InformaciÃ³n TÃ©cnica â”â”â”\n")
    print(f"DimensiÃ³n original:  {embeddings.shape[1]}")
    print(f"DimensiÃ³n PCA:       {encoder.n_dims_pca}")
    print(f"Ratio compresiÃ³n:    {embeddings.shape[1] / encoder.n_dims_pca:.2f}x")
    print(f"Varianza preservada: {encoder.pca.explained_variance_ratio_.sum():.3f}")
    
    print()

if __name__ == "__main__":
    test_semantic_preservation()
